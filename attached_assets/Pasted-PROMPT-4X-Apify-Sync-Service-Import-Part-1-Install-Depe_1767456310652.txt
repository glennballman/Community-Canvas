PROMPT 4X: Apify Sync Service + Import
Part 1: Install Dependencies
bashnpm install apify-client stream-json
Part 2: Sync Service
typescript// server/services/apifySync.ts
// Universal sync service for Apify datasets ‚Üí external_records ‚Üí entities

import { ApifyClient } from 'apify-client';
import { createReadStream } from 'fs';
import { parser } from 'stream-json';
import { streamArray } from 'stream-json/streamers/StreamArray';
import crypto from 'crypto';
import { pool } from '../db';

// =====================================================================
// TYPES
// =====================================================================

interface DatasetConfig {
    id: string;
    slug: string;
    source: string;
    record_type: string;
    region: string | null;
    apify_actor_id: string;
    apify_dataset_id: string | null;
}

interface SyncResult {
    success: boolean;
    recordsProcessed: number;
    recordsInserted: number;
    recordsUpdated: number;
    recordsSkipped: number;
    recordsErrored: number;
    entitiesCreated: number;
    durationSeconds: number;
    error?: string;
}

// =====================================================================
// HELPERS
// =====================================================================

function md5(data: any): string {
    return crypto.createHash('md5').update(JSON.stringify(data)).digest('hex');
}

function getNested(obj: any, path: string): any {
    return path.split('.').reduce((cur, key) => cur?.[key], obj);
}

function extractCoordinates(data: any): { lat: number | null; lng: number | null } {
    const formats = [
        { lat: 'coordinates.lat', lng: 'coordinates.lng' },
        { lat: 'coordinates.latitude', lng: 'coordinates.longitude' },
        { lat: 'location.lat', lng: 'location.lng' },
        { lat: 'lat', lng: 'lng' },
        { lat: 'latitude', lng: 'longitude' },
        { lat: 'geoLocation.lat', lng: 'geoLocation.lng' },
    ];
    
    for (const f of formats) {
        const lat = getNested(data, f.lat);
        const lng = getNested(data, f.lng);
        if (lat != null && lng != null) {
            return { lat: Number(lat), lng: Number(lng) };
        }
    }
    return { lat: null, lng: null };
}

function extractPrice(data: any): number | null {
    if (typeof data.price === 'number') return data.price;
    if (typeof data.price === 'string') return parseFloat(data.price) || null;
    if (data.price?.rate) return data.price.rate;
    if (data.price?.amount) return data.price.amount;
    if (data.pricing?.rate?.amount) return data.pricing.rate.amount;
    if (data.basePrice) return data.basePrice;
    return null;
}

function extractRating(data: any): { rating: number | null; reviewCount: number | null } {
    if (typeof data.rating === 'number') {
        return { rating: data.rating, reviewCount: data.reviewsCount || data.numberOfReviews || null };
    }
    if (data.rating?.value) {
        return { rating: data.rating.value, reviewCount: data.rating.reviewsCount || null };
    }
    if (data.stars) {
        return { rating: data.stars, reviewCount: data.numberOfReviews || null };
    }
    return { rating: null, reviewCount: null };
}

function extractPhotos(data: any): any[] {
    const photos = data.photos || data.images || data.pictures || [];
    return photos.slice(0, 30).map((p: any) => {
        if (typeof p === 'string') return { url: p, caption: '' };
        return {
            url: p.url || p.picture || p.large || p.original || '',
            caption: p.caption || p.label || ''
        };
    });
}

function extractBedroomsFromPhotos(photos: any[]): number | null {
    const bedroomPhotos = photos.filter(p => 
        (p.caption || '').toLowerCase().includes('bedroom')
    );
    return bedroomPhotos.length > 0 ? bedroomPhotos.length : null;
}

// =====================================================================
// RECORD PROCESSING
// =====================================================================

async function processRecord(
    record: any,
    config: DatasetConfig
): Promise<{ status: 'inserted' | 'updated' | 'skipped' | 'error'; recordId?: string }> {
    try {
        // Extract external ID (required)
        const externalId = String(
            record.id || record.listingId || record.roomId ||
            record.productId || record.sku || ''
        ).trim();
        
        if (!externalId) return { status: 'skipped' };
        
        const hash = md5(record);
        const coords = extractCoordinates(record);
        const { rating, reviewCount } = extractRating(record);
        const photos = extractPhotos(record);
        
        // Try to get bedrooms from data or photos
        let bedrooms = record.bedrooms || record.bedroomCount || null;
        if (!bedrooms && config.record_type === 'property_listing') {
            bedrooms = extractBedroomsFromPhotos(photos);
        }
        
        const name = String(record.title || record.name || record.productName || 'Unknown').slice(0, 500);
        const description = String(record.description || '').slice(0, 20000);
        const city = String(record.location?.city || record.city || record.address?.city || '').slice(0, 200);
        const region = String(record.location?.region || record.region || config.region || '').slice(0, 200);
        const url = String(record.url || record.listingUrl || '').slice(0, 2000);
        
        // Check if exists
        const existing = await pool.query(
            `SELECT id, sync_hash FROM external_records WHERE source = $1::external_source_enum AND external_id = $2`,
            [config.source, externalId]
        );
        
        if (existing.rows.length > 0) {
            const row = existing.rows[0];
            
            if (row.sync_hash === hash) {
                // No changes, just update last_seen
                await pool.query(
                    `UPDATE external_records SET last_seen_at = NOW() WHERE id = $1`,
                    [row.id]
                );
                return { status: 'skipped', recordId: row.id };
            }
            
            // Update existing record
            await pool.query(`
                UPDATE external_records SET
                    dataset_id = $1,
                    name = $2,
                    description = $3,
                    external_url = $4,
                    latitude = $5,
                    longitude = $6,
                    city = $7,
                    region = $8,
                    community_id = resolve_community($5, $6, $7, $8),
                    contact_email = $9,
                    contact_phone = $10,
                    contact_name = $11,
                    price = $12,
                    rating = $13,
                    review_count = $14,
                    max_occupancy = $15,
                    bedrooms = $16,
                    bathrooms = $17,
                    beds = $18,
                    property_type = $19,
                    amenities = $20,
                    photos = $21,
                    host_name = $22,
                    host_id = $23,
                    raw_data = $24,
                    sync_hash = $25,
                    last_seen_at = NOW(),
                    last_changed_at = NOW(),
                    updated_at = NOW()
                WHERE id = $26
            `, [
                config.id,
                name,
                description,
                url,
                coords.lat,
                coords.lng,
                city,
                region,
                record.host?.email || record.contactEmail || null,
                record.host?.phone || record.contactPhone || null,
                record.host?.name || record.hostName || null,
                extractPrice(record),
                rating,
                reviewCount,
                record.maxGuests || record.personCapacity || record.guestCapacity || null,
                bedrooms,
                record.bathrooms || record.bathroomCount || null,
                record.beds || record.bedCount || null,
                record.propertyType || record.roomType || null,
                JSON.stringify(record.amenities || []),
                JSON.stringify(photos),
                record.host?.name || record.hostName || '',
                String(record.host?.id || record.hostId || ''),
                JSON.stringify(record),
                hash,
                row.id
            ]);
            
            return { status: 'updated', recordId: row.id };
        }
        
        // Insert new record
        const inserted = await pool.query(`
            INSERT INTO external_records (
                dataset_id, source, record_type,
                external_id, external_url,
                name, description,
                latitude, longitude, city, region, country,
                community_id,
                contact_email, contact_phone, contact_name,
                price, currency, rating, review_count,
                max_occupancy, bedrooms, bathrooms, beds,
                property_type, amenities, photos,
                host_name, host_id,
                raw_data, sync_hash
            ) VALUES (
                $1, $2::external_source_enum, $3::external_record_type_enum,
                $4, $5,
                $6, $7,
                $8, $9, $10, $11, 'Canada',
                resolve_community($8, $9, $10, $11),
                $12, $13, $14,
                $15, 'CAD', $16, $17,
                $18, $19, $20, $21,
                $22, $23, $24,
                $25, $26,
                $27, $28
            )
            RETURNING id
        `, [
            config.id,
            config.source,
            config.record_type,
            externalId,
            url,
            name,
            description,
            coords.lat,
            coords.lng,
            city,
            region,
            record.host?.email || record.contactEmail || null,
            record.host?.phone || record.contactPhone || null,
            record.host?.name || record.hostName || null,
            extractPrice(record),
            rating,
            reviewCount,
            record.maxGuests || record.personCapacity || record.guestCapacity || null,
            bedrooms,
            record.bathrooms || record.bathroomCount || null,
            record.beds || record.bedCount || null,
            record.propertyType || record.roomType || null,
            JSON.stringify(record.amenities || []),
            JSON.stringify(photos),
            record.host?.name || record.hostName || '',
            String(record.host?.id || record.hostId || ''),
            JSON.stringify(record),
            hash
        ]);
        
        return { status: 'inserted', recordId: inserted.rows[0].id };
        
    } catch (error: any) {
        console.error('Error processing record:', error.message);
        return { status: 'error' };
    }
}

// =====================================================================
// AUTO-CREATE ENTITY FOR NEW RECORDS
// =====================================================================

async function createEntityForRecord(recordId: string): Promise<string | null> {
    try {
        const result = await pool.query(
            `SELECT create_entity_from_record($1)`,
            [recordId]
        );
        return result.rows[0]?.create_entity_from_record || null;
    } catch (error) {
        console.error('Error creating entity:', error);
        return null;
    }
}

// =====================================================================
// SYNC FROM APIFY API
// =====================================================================

export async function syncFromApify(datasetSlug: string): Promise<SyncResult> {
    const startTime = Date.now();
    
    // Get dataset config
    const configResult = await pool.query(
        `SELECT id, slug, source::text, record_type::text, region, apify_actor_id, apify_dataset_id
         FROM apify_datasets WHERE slug = $1`,
        [datasetSlug]
    );
    
    if (configResult.rows.length === 0) {
        throw new Error(`Dataset not found: ${datasetSlug}`);
    }
    
    const config: DatasetConfig = configResult.rows[0];
    
    // Create sync history
    const syncRecord = await pool.query(
        `INSERT INTO apify_sync_history (dataset_id, triggered_by) VALUES ($1, 'api') RETURNING id`,
        [config.id]
    );
    const syncId = syncRecord.rows[0].id;
    
    const result: SyncResult = {
        success: false,
        recordsProcessed: 0,
        recordsInserted: 0,
        recordsUpdated: 0,
        recordsSkipped: 0,
        recordsErrored: 0,
        entitiesCreated: 0,
        durationSeconds: 0
    };
    
    try {
        const apifyClient = new ApifyClient({
            token: process.env.APIFY_API_TOKEN
        });
        
        // Get dataset ID (from config or latest run)
        let datasetId = config.apify_dataset_id;
        
        if (!datasetId) {
            const runs = await apifyClient.actor(config.apify_actor_id).runs().list({ limit: 1 });
            if (runs.items.length === 0) {
                throw new Error('No runs found for actor');
            }
            datasetId = runs.items[0].defaultDatasetId;
        }
        
        console.log(`üì• Fetching from Apify dataset: ${datasetId}`);
        
        // Get items
        const dataset = apifyClient.dataset(datasetId);
        const { items } = await dataset.listItems();
        
        console.log(`üìä Processing ${items.length} records...`);
        
        for (const record of items) {
            result.recordsProcessed++;
            
            const { status, recordId } = await processRecord(record, config);
            
            switch (status) {
                case 'inserted':
                    result.recordsInserted++;
                    // Auto-create entity for new records
                    if (recordId) {
                        const entityId = await createEntityForRecord(recordId);
                        if (entityId) result.entitiesCreated++;
                    }
                    break;
                case 'updated':
                    result.recordsUpdated++;
                    break;
                case 'skipped':
                    result.recordsSkipped++;
                    break;
                case 'error':
                    result.recordsErrored++;
                    break;
            }
            
            if (result.recordsProcessed % 500 === 0) {
                console.log(`  ‚úì ${result.recordsProcessed.toLocaleString()} processed...`);
            }
        }
        
        result.success = true;
        
    } catch (error: any) {
        result.error = error.message;
        console.error('Sync failed:', error);
    }
    
    result.durationSeconds = Math.round((Date.now() - startTime) / 1000);
    
    // Update sync history
    await pool.query(`
        UPDATE apify_sync_history SET
            completed_at = NOW(),
            status = $2,
            records_processed = $3,
            records_inserted = $4,
            records_updated = $5,
            records_skipped = $6,
            records_errored = $7,
            duration_seconds = $8,
            error_message = $9
        WHERE id = $1
    `, [
        syncId,
        result.success ? 'completed' : 'failed',
        result.recordsProcessed,
        result.recordsInserted,
        result.recordsUpdated,
        result.recordsSkipped,
        result.recordsErrored,
        result.durationSeconds,
        result.error
    ]);
    
    // Update dataset
    await pool.query(`
        UPDATE apify_datasets SET 
            last_sync_at = NOW(),
            last_sync_record_count = $2,
            last_sync_error = $3
        WHERE id = $1
    `, [config.id, result.recordsProcessed, result.error]);
    
    return result;
}

// =====================================================================
// SYNC FROM LOCAL JSON FILE (for large 343MB files)
// =====================================================================

export async function syncFromFile(
    datasetSlug: string,
    filePath: string
): Promise<SyncResult> {
    const startTime = Date.now();
    
    // Get dataset config
    const configResult = await pool.query(
        `SELECT id, slug, source::text, record_type::text, region, apify_actor_id, apify_dataset_id
         FROM apify_datasets WHERE slug = $1`,
        [datasetSlug]
    );
    
    if (configResult.rows.length === 0) {
        throw new Error(`Dataset not found: ${datasetSlug}`);
    }
    
    const config: DatasetConfig = configResult.rows[0];
    
    // Create sync history
    const syncRecord = await pool.query(
        `INSERT INTO apify_sync_history (dataset_id, triggered_by) VALUES ($1, 'file') RETURNING id`,
        [config.id]
    );
    const syncId = syncRecord.rows[0].id;
    
    const result: SyncResult = {
        success: false,
        recordsProcessed: 0,
        recordsInserted: 0,
        recordsUpdated: 0,
        recordsSkipped: 0,
        recordsErrored: 0,
        entitiesCreated: 0,
        durationSeconds: 0
    };
    
    try {
        console.log(`üìÅ Reading from: ${filePath}`);
        
        const fileStream = createReadStream(filePath);
        const jsonStream = fileStream.pipe(parser()).pipe(streamArray());
        
        for await (const { value: record } of jsonStream) {
            result.recordsProcessed++;
            
            const { status, recordId } = await processRecord(record, config);
            
            switch (status) {
                case 'inserted':
                    result.recordsInserted++;
                    if (recordId) {
                        const entityId = await createEntityForRecord(recordId);
                        if (entityId) result.entitiesCreated++;
                    }
                    break;
                case 'updated':
                    result.recordsUpdated++;
                    break;
                case 'skipped':
                    result.recordsSkipped++;
                    break;
                case 'error':
                    result.recordsErrored++;
                    break;
            }
            
            if (result.recordsProcessed % 1000 === 0) {
                const elapsed = ((Date.now() - startTime) / 1000).toFixed(1);
                const rate = (result.recordsProcessed / parseFloat(elapsed)).toFixed(0);
                console.log(`  ‚úì ${result.recordsProcessed.toLocaleString()} processed (${rate}/sec)...`);
            }
        }
        
        result.success = true;
        
    } catch (error: any) {
        result.error = error.message;
        console.error('Sync from file failed:', error);
    }
    
    result.durationSeconds = Math.round((Date.now() - startTime) / 1000);
    
    // Update sync history
    await pool.query(`
        UPDATE apify_sync_history SET
            completed_at = NOW(),
            status = $2,
            records_processed = $3,
            records_inserted = $4,
            records_updated = $5,
            records_skipped = $6,
            records_errored = $7,
            duration_seconds = $8,
            error_message = $9
        WHERE id = $1
    `, [
        syncId,
        result.success ? 'completed' : 'failed',
        result.recordsProcessed,
        result.recordsInserted,
        result.recordsUpdated,
        result.recordsSkipped,
        result.recordsErrored,
        result.durationSeconds,
        result.error
    ]);
    
    // Update dataset
    await pool.query(`
        UPDATE apify_datasets SET 
            last_sync_at = NOW(),
            last_sync_record_count = $2,
            last_sync_error = $3
        WHERE id = $1
    `, [config.id, result.recordsProcessed, result.error]);
    
    return result;
}

// =====================================================================
// EXPORT FUNCTIONS
// =====================================================================

export async function getDatasetStatus() {
    const result = await pool.query(`SELECT * FROM v_dataset_sync_status`);
    return result.rows;
}

export async function getEntityStats() {
    const result = await pool.query(`SELECT * FROM v_entity_stats`);
    return result.rows;
}
Part 3: API Routes
typescript// server/routes/apify.ts

import { Router } from 'express';
import { pool } from '../db';
import { syncFromApify, syncFromFile, getDatasetStatus, getEntityStats } from '../services/apifySync';
import { authenticateToken } from '../middleware/auth';

const router = Router();

// GET /api/apify/datasets - List all datasets with sync status
router.get('/datasets', authenticateToken, async (req, res) => {
    try {
        const datasets = await getDatasetStatus();
        res.json({ success: true, datasets });
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

// POST /api/apify/sync/:slug - Trigger sync
router.post('/sync/:slug', authenticateToken, async (req, res) => {
    try {
        const { slug } = req.params;
        const { filePath } = req.body;
        
        console.log(`üöÄ Starting sync for ${slug}...`);
        
        // Run sync (this could be long-running)
        const result = filePath 
            ? await syncFromFile(slug, filePath)
            : await syncFromApify(slug);
        
        console.log(`‚úÖ Sync completed:`, result);
        
        res.json({ success: true, result });
        
    } catch (error: any) {
        console.error('Sync error:', error);
        res.status(500).json({ success: false, error: error.message });
    }
});

// GET /api/apify/stats - Entity statistics
router.get('/stats', authenticateToken, async (req, res) => {
    try {
        const stats = await getEntityStats();
        
        // Also get totals
        const totals = await pool.query(`
            SELECT 
                (SELECT COUNT(*) FROM external_records) as total_records,
                (SELECT COUNT(*) FROM entities) as total_entities,
                (SELECT COUNT(*) FROM entities WHERE claim_status = 'approved') as claimed_entities,
                (SELECT COUNT(*) FROM entity_inquiries WHERE status = 'pending') as pending_inquiries
        `);
        
        res.json({ 
            success: true, 
            stats,
            totals: totals.rows[0]
        });
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

// GET /api/apify/records - Browse external records
router.get('/records', authenticateToken, async (req, res) => {
    try {
        const { source, type, community, search, limit = 50, offset = 0 } = req.query;
        
        let query = `
            SELECT 
                er.*,
                c.name as community_name,
                (SELECT COUNT(*) FROM entity_links el WHERE el.external_record_id = er.id AND el.status = 'accepted') as linked
            FROM external_records er
            LEFT JOIN sr_communities c ON c.id = er.community_id
            WHERE 1=1
        `;
        const params: any[] = [];
        let paramIndex = 1;
        
        if (source) {
            query += ` AND er.source = $${paramIndex++}::external_source_enum`;
            params.push(source);
        }
        if (type) {
            query += ` AND er.record_type = $${paramIndex++}::external_record_type_enum`;
            params.push(type);
        }
        if (community) {
            query += ` AND er.community_id = $${paramIndex++}`;
            params.push(community);
        }
        if (search) {
            query += ` AND er.name ILIKE $${paramIndex++}`;
            params.push(`%${search}%`);
        }
        
        query += ` ORDER BY er.last_seen_at DESC LIMIT $${paramIndex++} OFFSET $${paramIndex++}`;
        params.push(limit, offset);
        
        const result = await pool.query(query, params);
        
        // Get total count
        let countQuery = `SELECT COUNT(*) FROM external_records er WHERE 1=1`;
        const countParams: any[] = [];
        let countParamIndex = 1;
        
        if (source) {
            countQuery += ` AND er.source = $${countParamIndex++}::external_source_enum`;
            countParams.push(source);
        }
        if (type) {
            countQuery += ` AND er.record_type = $${countParamIndex++}::external_record_type_enum`;
            countParams.push(type);
        }
        if (community) {
            countQuery += ` AND er.community_id = $${countParamIndex++}`;
            countParams.push(community);
        }
        if (search) {
            countQuery += ` AND er.name ILIKE $${countParamIndex++}`;
            countParams.push(`%${search}%`);
        }
        
        const countResult = await pool.query(countQuery, countParams);
        
        res.json({ 
            success: true, 
            records: result.rows,
            total: parseInt(countResult.rows[0].count),
            limit: Number(limit),
            offset: Number(offset)
        });
        
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

// GET /api/apify/entities - Browse canonical entities
router.get('/entities', authenticateToken, async (req, res) => {
    try {
        const { type, claimStatus, community, search, limit = 50, offset = 0 } = req.query;
        
        let query = `
            SELECT 
                e.*,
                c.name as community_name,
                (SELECT COUNT(*) FROM entity_links el WHERE el.entity_id = e.id AND el.status = 'accepted') as record_count,
                (SELECT COUNT(*) FROM entity_inquiries ei WHERE ei.entity_id = e.id AND ei.status = 'pending') as pending_inquiries
            FROM entities e
            LEFT JOIN sr_communities c ON c.id = e.community_id
            WHERE 1=1
        `;
        const params: any[] = [];
        let paramIndex = 1;
        
        if (type) {
            query += ` AND e.entity_type = $${paramIndex++}::entity_type_enum`;
            params.push(type);
        }
        if (claimStatus) {
            query += ` AND e.claim_status = $${paramIndex++}::claim_status_enum`;
            params.push(claimStatus);
        }
        if (community) {
            query += ` AND e.community_id = $${paramIndex++}`;
            params.push(community);
        }
        if (search) {
            query += ` AND e.display_name ILIKE $${paramIndex++}`;
            params.push(`%${search}%`);
        }
        
        query += ` ORDER BY e.created_at DESC LIMIT $${paramIndex++} OFFSET $${paramIndex++}`;
        params.push(limit, offset);
        
        const result = await pool.query(query, params);
        
        res.json({ 
            success: true, 
            entities: result.rows
        });
        
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

export default router;
Part 4: Register Routes
typescript// In server/index.ts or server/routes/index.ts, add:

import apifyRoutes from './routes/apify';

app.use('/api/apify', apifyRoutes);
Part 5: Import Script (for 343MB file)
typescript// server/scripts/importApifyBC.ts
// Run with: npx ts-node server/scripts/importApifyBC.ts

import { syncFromFile } from '../services/apifySync';

async function main() {
    console.log('üöÄ Starting BC Airbnb import...\n');
    
    const result = await syncFromFile(
        'airbnb-bc',
        './data/apify-airbnb-bc-complete.json'  // Your 343MB file
    );
    
    console.log('\n‚úÖ Import complete!\n');
    console.log(`   Records processed: ${result.recordsProcessed.toLocaleString()}`);
    console.log(`   Records inserted:  ${result.recordsInserted.toLocaleString()}`);
    console.log(`   Records updated:   ${result.recordsUpdated.toLocaleString()}`);
    console.log(`   Records skipped:   ${result.recordsSkipped.toLocaleString()}`);
    console.log(`   Records errored:   ${result.recordsErrored.toLocaleString()}`);
    console.log(`   Entities created:  ${result.entitiesCreated.toLocaleString()}`);
    console.log(`   Duration:          ${result.durationSeconds}s`);
    
    if (result.error) {
        console.error(`\n‚ùå Error: ${result.error}`);
    }
    
    process.exit(result.success ? 0 : 1);
}

main();
```

---

## How to Import Your 12,795 Listings

**Option A: Upload JSON to Replit**
1. Upload your 343MB JSON to Replit `/data/apify-airbnb-bc-complete.json`
2. Run: `npx ts-node server/scripts/importApifyBC.ts`

**Option B: Connect Apify API**
1. Set `APIFY_API_TOKEN` in Replit secrets
2. Update the dataset record with your `apify_dataset_id`
3. Call: `POST /api/apify/sync/airbnb-bc`

---

## Expected Output
```
üöÄ Starting BC Airbnb import...

üìÅ Reading from: ./data/apify-airbnb-bc-complete.json
  ‚úì 1,000 processed (142/sec)...
  ‚úì 2,000 processed (156/sec)...
  ...
  ‚úì 12,000 processed (148/sec)...

‚úÖ Import complete!

   Records processed: 12,795
   Records inserted:  12,743
   Records updated:   0
   Records skipped:   52
   Records errored:   0
   Entities created:  12,743
   Duration:          86s
Ready to import!