Let's Reconcile - Extend the V2 Schema
The V2 architecture is better (separate contact points, consent tracking). Let's extend it with the rental-specific fields and sync service.
PROMPT 4X-Reconciled: Extend Schema + Sync Service
Part 1: Extend external_records with Rental Fields
sql-- =====================================================================
-- EXTEND external_records FOR RENTAL DATA
-- Adds fields needed for STR/equipment without breaking V2 architecture
-- =====================================================================

-- Sync tracking
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS external_url TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS sync_hash TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS first_seen_at TIMESTAMPTZ DEFAULT NOW();
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS last_seen_at TIMESTAMPTZ DEFAULT NOW();
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS last_changed_at TIMESTAMPTZ;

-- Pricing
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS price DECIMAL(10,2);
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS currency TEXT DEFAULT 'CAD';

-- Ratings
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS rating DECIMAL(3,2);
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS review_count INTEGER;

-- Property-specific (STR)
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS max_occupancy INTEGER;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS bedrooms INTEGER;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS bathrooms DECIMAL(3,1);
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS beds INTEGER;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS property_type TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS amenities JSONB DEFAULT '[]';
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS photos JSONB DEFAULT '[]';

-- Host info
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS host_name TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS host_id TEXT;

-- Product-specific (Canadian Tire, Home Depot)
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS brand TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS sku TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS category TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS in_stock BOOLEAN;

-- Contact fields (extracted, will also go to contact_points with consent=unknown)
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS contact_email TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS contact_phone TEXT;
ALTER TABLE external_records ADD COLUMN IF NOT EXISTS contact_name TEXT;

-- Indexes for performance
CREATE INDEX IF NOT EXISTS idx_external_records_sync_hash ON external_records(sync_hash);
CREATE INDEX IF NOT EXISTS idx_external_records_last_seen ON external_records(last_seen_at DESC);
CREATE INDEX IF NOT EXISTS idx_external_records_city ON external_records(city);

-- =====================================================================
-- ADD record_type ENUM IF NOT EXISTS
-- =====================================================================

DO $$ BEGIN
  CREATE TYPE external_record_type_enum AS ENUM (
    'property_listing',
    'host_profile',
    'equipment_listing',
    'service_provider',
    'business_listing',
    'retail_product',
    'poi',
    'person_profile',
    'other'
  );
EXCEPTION WHEN duplicate_object THEN NULL; END $$;

ALTER TABLE external_records ADD COLUMN IF NOT EXISTS record_type external_record_type_enum DEFAULT 'other';

-- =====================================================================
-- CREATE ENTITY FROM RECORD (SQL function)
-- =====================================================================

CREATE OR REPLACE FUNCTION create_entity_from_record(p_record_id UUID)
RETURNS UUID AS $$
DECLARE
    v_record RECORD;
    v_entity_id UUID;
    v_entity_type entity_type;
BEGIN
    SELECT * INTO v_record FROM external_records WHERE id = p_record_id;
    
    IF v_record IS NULL THEN
        RETURN NULL;
    END IF;
    
    -- Map record type to entity type
    v_entity_type := CASE v_record.record_type
        WHEN 'property_listing' THEN 'property'
        WHEN 'host_profile' THEN 'person'
        WHEN 'equipment_listing' THEN 'equipment'
        WHEN 'service_provider' THEN 'service_provider'
        WHEN 'business_listing' THEN 'organization'
        WHEN 'retail_product' THEN 'product'
        WHEN 'person_profile' THEN 'person'
        ELSE 'other'
    END;
    
    -- Create entity
    INSERT INTO entities (
        entity_type, 
        display_name, 
        description,
        address, 
        city, 
        region, 
        country,
        latitude, 
        longitude, 
        community_id,
        primary_email, 
        primary_phone,
        merged_photos, 
        merged_amenities, 
        merged_rating, 
        merged_review_count,
        external_record_count
    ) VALUES (
        v_entity_type, 
        COALESCE(v_record.name, 'Unknown'),
        v_record.description,
        v_record.address, 
        v_record.city, 
        v_record.region, 
        COALESCE(v_record.country, 'Canada'),
        v_record.latitude, 
        v_record.longitude, 
        v_record.community_id,
        v_record.contact_email, 
        v_record.contact_phone,
        COALESCE(v_record.photos, '[]'::jsonb), 
        COALESCE(v_record.amenities, '[]'::jsonb), 
        v_record.rating, 
        v_record.review_count,
        1
    )
    RETURNING id INTO v_entity_id;
    
    -- Create accepted link
    INSERT INTO entity_links (external_record_id, entity_id, status, confidence, match_reasons)
    VALUES (p_record_id, v_entity_id, 'accepted', 1.0, '{"reason": "created_from_record"}'::jsonb);
    
    -- Also create contact point if email exists (with unknown consent)
    IF v_record.contact_email IS NOT NULL AND v_record.contact_email != '' THEN
        INSERT INTO external_contact_points (
            external_record_id, 
            contact_type, 
            contact_value, 
            normalized_value,
            consent, 
            do_not_contact
        ) VALUES (
            p_record_id,
            'email',
            v_record.contact_email,
            LOWER(TRIM(v_record.contact_email)),
            'unknown',
            false  -- Can contact for legitimate inquiries
        )
        ON CONFLICT (contact_type, normalized_value) DO NOTHING;
    END IF;
    
    RETURN v_entity_id;
END;
$$ LANGUAGE plpgsql;

-- =====================================================================
-- RESOLVE COMMUNITY (proper PostGIS if available)
-- =====================================================================

CREATE OR REPLACE FUNCTION resolve_community(
    p_lat DOUBLE PRECISION,
    p_lng DOUBLE PRECISION,
    p_city TEXT DEFAULT NULL,
    p_region TEXT DEFAULT NULL
) RETURNS UUID AS $$
DECLARE
    v_community_id UUID;
BEGIN
    -- First try exact city match
    IF p_city IS NOT NULL AND p_city != '' THEN
        SELECT id INTO v_community_id
        FROM sr_communities
        WHERE LOWER(name) = LOWER(TRIM(p_city))
        AND (p_region IS NULL OR p_region = '' OR LOWER(region) = LOWER(TRIM(p_region)))
        LIMIT 1;
        
        IF v_community_id IS NOT NULL THEN
            RETURN v_community_id;
        END IF;
    END IF;
    
    -- Fall back to nearest by coordinates
    IF p_lat IS NOT NULL AND p_lng IS NOT NULL THEN
        -- Try PostGIS first
        BEGIN
            SELECT id INTO v_community_id
            FROM sr_communities
            WHERE geom IS NOT NULL
            ORDER BY geom <-> ST_SetSRID(ST_MakePoint(p_lng, p_lat), 4326)::geography
            LIMIT 1;
            
            IF v_community_id IS NOT NULL THEN
                RETURN v_community_id;
            END IF;
        EXCEPTION WHEN OTHERS THEN
            -- PostGIS not available, fall back to Euclidean
            NULL;
        END;
        
        -- Euclidean fallback
        SELECT id INTO v_community_id
        FROM sr_communities
        WHERE latitude IS NOT NULL AND longitude IS NOT NULL
        ORDER BY SQRT(POWER(latitude - p_lat, 2) + POWER(longitude - p_lng, 2))
        LIMIT 1;
    END IF;
    
    RETURN v_community_id;
END;
$$ LANGUAGE plpgsql;

-- =====================================================================
-- VIEWS
-- =====================================================================

-- Dataset sync status
CREATE OR REPLACE VIEW v_dataset_sync_status AS
SELECT 
    d.name,
    d.slug,
    d.source::text,
    d.record_type::text,
    d.region,
    d.sync_enabled,
    d.last_sync_at,
    d.last_sync_record_count,
    (SELECT COUNT(*) FROM external_records WHERE dataset_id = d.id) as total_records,
    (SELECT COUNT(*) FROM external_records er 
     JOIN entity_links el ON el.external_record_id = er.id AND el.status = 'accepted'
     WHERE er.dataset_id = d.id) as resolved_records,
    CASE 
        WHEN d.last_sync_at IS NULL THEN 'Never synced'
        WHEN d.last_sync_at < NOW() - (d.sync_frequency_hours || ' hours')::interval THEN 'Overdue'
        ELSE 'OK'
    END as sync_health
FROM apify_datasets d
ORDER BY d.source, d.name;

-- Entity statistics by source
CREATE OR REPLACE VIEW v_entity_stats AS
SELECT 
    er.source::text,
    er.record_type::text,
    COUNT(DISTINCT er.id) as total_records,
    COUNT(DISTINCT el.entity_id) FILTER (WHERE el.status = 'accepted') as resolved_to_entities,
    COUNT(DISTINCT e.id) FILTER (WHERE e.claim_status = 'approved') as claimed_entities
FROM external_records er
LEFT JOIN entity_links el ON el.external_record_id = er.id
LEFT JOIN entities e ON e.id = el.entity_id
GROUP BY er.source, er.record_type
ORDER BY er.source, er.record_type;

-- Unresolved records needing entity assignment
CREATE OR REPLACE VIEW v_unresolved_records AS
SELECT 
    er.id,
    er.source::text,
    er.record_type::text,
    er.name,
    er.city,
    er.community_id,
    c.name as community_name,
    er.first_seen_at
FROM external_records er
LEFT JOIN sr_communities c ON c.id = er.community_id
WHERE NOT EXISTS (
    SELECT 1 FROM entity_links el 
    WHERE el.external_record_id = er.id AND el.status = 'accepted'
)
ORDER BY er.first_seen_at DESC;

-- =====================================================================
-- UPDATE apify_datasets with record_type column
-- =====================================================================

ALTER TABLE apify_datasets ADD COLUMN IF NOT EXISTS record_type external_record_type_enum DEFAULT 'other';

-- Update existing datasets
UPDATE apify_datasets SET record_type = 'property_listing' WHERE slug LIKE 'airbnb%' OR slug LIKE 'vrbo%';
UPDATE apify_datasets SET record_type = 'retail_product' WHERE slug IN ('canadian-tire', 'home-depot');
UPDATE apify_datasets SET record_type = 'business_listing' WHERE slug LIKE 'chamber%';

-- =====================================================================
-- VERIFY
-- =====================================================================

SELECT 'Schema extended for rental data' as status;

SELECT column_name, data_type 
FROM information_schema.columns 
WHERE table_name = 'external_records' 
ORDER BY ordinal_position;
Part 2: Updated Sync Service (matches V2 schema)
typescript// server/services/apifySync.ts
// Sync service compatible with V2 external_records schema

import { createReadStream } from 'fs';
import { pool } from '../db';

// Only import these if using Apify API directly
let ApifyClient: any;
let parser: any;
let streamArray: any;

// =====================================================================
// TYPES
// =====================================================================

interface DatasetConfig {
    id: string;
    slug: string;
    source: string;
    record_type: string;
    region: string | null;
    apify_actor_id: string;
    apify_dataset_id: string | null;
}

interface SyncResult {
    success: boolean;
    recordsProcessed: number;
    recordsInserted: number;
    recordsUpdated: number;
    recordsSkipped: number;
    recordsErrored: number;
    entitiesCreated: number;
    durationSeconds: number;
    error?: string;
}

// =====================================================================
// HELPERS
// =====================================================================

function md5(data: any): string {
    const crypto = require('crypto');
    return crypto.createHash('md5').update(JSON.stringify(data)).digest('hex');
}

function getNested(obj: any, path: string): any {
    return path.split('.').reduce((cur, key) => cur?.[key], obj);
}

function extractCoordinates(data: any): { lat: number | null; lng: number | null } {
    const formats = [
        { lat: 'coordinates.lat', lng: 'coordinates.lng' },
        { lat: 'coordinates.latitude', lng: 'coordinates.longitude' },
        { lat: 'location.lat', lng: 'location.lng' },
        { lat: 'lat', lng: 'lng' },
        { lat: 'latitude', lng: 'longitude' },
        { lat: 'geoLocation.lat', lng: 'geoLocation.lng' },
    ];
    
    for (const f of formats) {
        const lat = getNested(data, f.lat);
        const lng = getNested(data, f.lng);
        if (lat != null && lng != null) {
            return { lat: Number(lat), lng: Number(lng) };
        }
    }
    return { lat: null, lng: null };
}

function extractPrice(data: any): number | null {
    if (typeof data.price === 'number') return data.price;
    if (typeof data.price === 'string') return parseFloat(data.price) || null;
    if (data.price?.rate) return data.price.rate;
    if (data.price?.amount) return data.price.amount;
    if (data.pricing?.rate?.amount) return data.pricing.rate.amount;
    if (data.basePrice) return data.basePrice;
    return null;
}

function extractRating(data: any): { rating: number | null; reviewCount: number | null } {
    if (typeof data.rating === 'number') {
        return { rating: data.rating, reviewCount: data.reviewsCount || data.numberOfReviews || null };
    }
    if (data.rating?.value) {
        return { rating: data.rating.value, reviewCount: data.rating.reviewsCount || null };
    }
    if (data.stars) {
        return { rating: data.stars, reviewCount: data.numberOfReviews || null };
    }
    return { rating: null, reviewCount: null };
}

function extractPhotos(data: any): any[] {
    const photos = data.photos || data.images || data.pictures || [];
    return photos.slice(0, 30).map((p: any) => {
        if (typeof p === 'string') return { url: p, caption: '' };
        return {
            url: p.url || p.picture || p.large || p.original || '',
            caption: p.caption || p.label || ''
        };
    });
}

function extractBedroomsFromPhotos(photos: any[]): number | null {
    const bedroomPhotos = photos.filter(p => 
        (p.caption || '').toLowerCase().includes('bedroom')
    );
    return bedroomPhotos.length > 0 ? bedroomPhotos.length : null;
}

// =====================================================================
// RECORD PROCESSING
// =====================================================================

async function processRecord(
    record: any,
    config: DatasetConfig
): Promise<{ status: 'inserted' | 'updated' | 'skipped' | 'error'; recordId?: string }> {
    try {
        // Extract external ID (required)
        const externalId = String(
            record.id || record.listingId || record.roomId ||
            record.productId || record.sku || ''
        ).trim();
        
        if (!externalId) return { status: 'skipped' };
        
        const hash = md5(record);
        const coords = extractCoordinates(record);
        const { rating, reviewCount } = extractRating(record);
        const photos = extractPhotos(record);
        
        // Try to get bedrooms from data or photos
        let bedrooms = record.bedrooms || record.bedroomCount || null;
        if (!bedrooms && config.record_type === 'property_listing') {
            bedrooms = extractBedroomsFromPhotos(photos);
        }
        
        const name = String(record.title || record.name || record.productName || 'Unknown').slice(0, 500);
        const description = String(record.description || '').slice(0, 20000);
        const city = String(record.location?.city || record.city || record.address?.city || '').slice(0, 200);
        const region = String(record.location?.region || record.region || config.region || '').slice(0, 200);
        const url = String(record.url || record.listingUrl || '').slice(0, 2000);
        
        // Check if exists
        const existing = await pool.query(
            `SELECT id, sync_hash FROM external_records WHERE source = $1::data_source_type AND external_id = $2`,
            [config.source, externalId]
        );
        
        if (existing.rows.length > 0) {
            const row = existing.rows[0];
            
            if (row.sync_hash === hash) {
                // No changes, just update last_seen
                await pool.query(
                    `UPDATE external_records SET last_seen_at = NOW() WHERE id = $1`,
                    [row.id]
                );
                return { status: 'skipped', recordId: row.id };
            }
            
            // Update existing record
            await pool.query(`
                UPDATE external_records SET
                    dataset_id = $1,
                    name = $2,
                    description = $3,
                    external_url = $4,
                    latitude = $5,
                    longitude = $6,
                    city = $7,
                    region = $8,
                    community_id = resolve_community($5, $6, $7, $8),
                    contact_email = $9,
                    contact_phone = $10,
                    contact_name = $11,
                    price = $12,
                    rating = $13,
                    review_count = $14,
                    max_occupancy = $15,
                    bedrooms = $16,
                    bathrooms = $17,
                    beds = $18,
                    property_type = $19,
                    amenities = $20,
                    photos = $21,
                    host_name = $22,
                    host_id = $23,
                    raw_data = $24,
                    sync_hash = $25,
                    last_seen_at = NOW(),
                    last_changed_at = NOW(),
                    updated_at = NOW()
                WHERE id = $26
            `, [
                config.id,
                name,
                description,
                url,
                coords.lat,
                coords.lng,
                city,
                region,
                record.host?.email || record.contactEmail || null,
                record.host?.phone || record.contactPhone || null,
                record.host?.name || record.hostName || null,
                extractPrice(record),
                rating,
                reviewCount,
                record.maxGuests || record.personCapacity || record.guestCapacity || null,
                bedrooms,
                record.bathrooms || record.bathroomCount || null,
                record.beds || record.bedCount || null,
                record.propertyType || record.roomType || null,
                JSON.stringify(record.amenities || []),
                JSON.stringify(photos),
                record.host?.name || record.hostName || '',
                String(record.host?.id || record.hostId || ''),
                JSON.stringify(record),
                hash,
                row.id
            ]);
            
            return { status: 'updated', recordId: row.id };
        }
        
        // Insert new record
        const inserted = await pool.query(`
            INSERT INTO external_records (
                dataset_id, source, record_type,
                external_id, external_url,
                name, description,
                latitude, longitude, city, region, country,
                community_id,
                contact_email, contact_phone, contact_name,
                price, currency, rating, review_count,
                max_occupancy, bedrooms, bathrooms, beds,
                property_type, amenities, photos,
                host_name, host_id,
                raw_data, sync_hash
            ) VALUES (
                $1, $2::data_source_type, $3::external_record_type_enum,
                $4, $5,
                $6, $7,
                $8, $9, $10, $11, 'Canada',
                resolve_community($8, $9, $10, $11),
                $12, $13, $14,
                $15, 'CAD', $16, $17,
                $18, $19, $20, $21,
                $22, $23, $24,
                $25, $26,
                $27, $28
            )
            RETURNING id
        `, [
            config.id,
            config.source,
            config.record_type,
            externalId,
            url,
            name,
            description,
            coords.lat,
            coords.lng,
            city,
            region,
            record.host?.email || record.contactEmail || null,
            record.host?.phone || record.contactPhone || null,
            record.host?.name || record.hostName || null,
            extractPrice(record),
            rating,
            reviewCount,
            record.maxGuests || record.personCapacity || record.guestCapacity || null,
            bedrooms,
            record.bathrooms || record.bathroomCount || null,
            record.beds || record.bedCount || null,
            record.propertyType || record.roomType || null,
            JSON.stringify(record.amenities || []),
            JSON.stringify(photos),
            record.host?.name || record.hostName || '',
            String(record.host?.id || record.hostId || ''),
            JSON.stringify(record),
            hash
        ]);
        
        return { status: 'inserted', recordId: inserted.rows[0].id };
        
    } catch (error: any) {
        console.error('Error processing record:', error.message);
        return { status: 'error' };
    }
}

// =====================================================================
// AUTO-CREATE ENTITY
// =====================================================================

async function createEntityForRecord(recordId: string): Promise<string | null> {
    try {
        const result = await pool.query(
            `SELECT create_entity_from_record($1)`,
            [recordId]
        );
        return result.rows[0]?.create_entity_from_record || null;
    } catch (error) {
        console.error('Error creating entity:', error);
        return null;
    }
}

// =====================================================================
// SYNC FROM JSON FILE (for large files)
// =====================================================================

export async function syncFromFile(
    datasetSlug: string,
    filePath: string
): Promise<SyncResult> {
    // Lazy load stream-json
    if (!parser) {
        const streamJson = await import('stream-json');
        const streamers = await import('stream-json/streamers/StreamArray');
        parser = streamJson.parser;
        streamArray = streamers.streamArray;
    }
    
    const startTime = Date.now();
    
    // Get dataset config
    const configResult = await pool.query(
        `SELECT id, slug, source::text, record_type::text, region, apify_actor_id, apify_dataset_id
         FROM apify_datasets WHERE slug = $1`,
        [datasetSlug]
    );
    
    if (configResult.rows.length === 0) {
        throw new Error(`Dataset not found: ${datasetSlug}`);
    }
    
    const config: DatasetConfig = configResult.rows[0];
    
    // Create sync history
    const syncRecord = await pool.query(
        `INSERT INTO apify_sync_history (dataset_id, triggered_by) VALUES ($1, 'file') RETURNING id`,
        [config.id]
    );
    const syncId = syncRecord.rows[0].id;
    
    const result: SyncResult = {
        success: false,
        recordsProcessed: 0,
        recordsInserted: 0,
        recordsUpdated: 0,
        recordsSkipped: 0,
        recordsErrored: 0,
        entitiesCreated: 0,
        durationSeconds: 0
    };
    
    try {
        console.log(`üìÅ Reading from: ${filePath}`);
        
        const fileStream = createReadStream(filePath);
        const jsonStream = fileStream.pipe(parser()).pipe(streamArray());
        
        for await (const { value: record } of jsonStream) {
            result.recordsProcessed++;
            
            const { status, recordId } = await processRecord(record, config);
            
            switch (status) {
                case 'inserted':
                    result.recordsInserted++;
                    if (recordId) {
                        const entityId = await createEntityForRecord(recordId);
                        if (entityId) result.entitiesCreated++;
                    }
                    break;
                case 'updated':
                    result.recordsUpdated++;
                    break;
                case 'skipped':
                    result.recordsSkipped++;
                    break;
                case 'error':
                    result.recordsErrored++;
                    break;
            }
            
            if (result.recordsProcessed % 1000 === 0) {
                const elapsed = ((Date.now() - startTime) / 1000).toFixed(1);
                const rate = (result.recordsProcessed / parseFloat(elapsed)).toFixed(0);
                console.log(`  ‚úì ${result.recordsProcessed.toLocaleString()} processed (${rate}/sec)...`);
            }
        }
        
        result.success = true;
        
    } catch (error: any) {
        result.error = error.message;
        console.error('Sync from file failed:', error);
    }
    
    result.durationSeconds = Math.round((Date.now() - startTime) / 1000);
    
    // Update sync history
    await pool.query(`
        UPDATE apify_sync_history SET
            completed_at = NOW(),
            status = $2,
            records_processed = $3,
            records_inserted = $4,
            records_updated = $5,
            records_skipped = $6,
            records_errored = $7,
            duration_seconds = $8,
            error_message = $9
        WHERE id = $1
    `, [
        syncId,
        result.success ? 'completed' : 'failed',
        result.recordsProcessed,
        result.recordsInserted,
        result.recordsUpdated,
        result.recordsSkipped,
        result.recordsErrored,
        result.durationSeconds,
        result.error
    ]);
    
    // Update dataset
    await pool.query(`
        UPDATE apify_datasets SET 
            last_sync_at = NOW(),
            last_sync_record_count = $2,
            last_sync_error = $3
        WHERE id = $1
    `, [config.id, result.recordsProcessed, result.error]);
    
    return result;
}

// =====================================================================
// SYNC FROM APIFY API
// =====================================================================

export async function syncFromApify(datasetSlug: string): Promise<SyncResult> {
    // Lazy load apify-client
    if (!ApifyClient) {
        const apify = await import('apify-client');
        ApifyClient = apify.ApifyClient;
    }
    
    const startTime = Date.now();
    
    // Get dataset config
    const configResult = await pool.query(
        `SELECT id, slug, source::text, record_type::text, region, apify_actor_id, apify_dataset_id
         FROM apify_datasets WHERE slug = $1`,
        [datasetSlug]
    );
    
    if (configResult.rows.length === 0) {
        throw new Error(`Dataset not found: ${datasetSlug}`);
    }
    
    const config: DatasetConfig = configResult.rows[0];
    
    // Create sync history
    const syncRecord = await pool.query(
        `INSERT INTO apify_sync_history (dataset_id, triggered_by) VALUES ($1, 'api') RETURNING id`,
        [config.id]
    );
    const syncId = syncRecord.rows[0].id;
    
    const result: SyncResult = {
        success: false,
        recordsProcessed: 0,
        recordsInserted: 0,
        recordsUpdated: 0,
        recordsSkipped: 0,
        recordsErrored: 0,
        entitiesCreated: 0,
        durationSeconds: 0
    };
    
    try {
        const client = new ApifyClient({
            token: process.env.APIFY_API_TOKEN
        });
        
        // Get dataset ID
        let datasetId = config.apify_dataset_id;
        
        if (!datasetId) {
            const runs = await client.actor(config.apify_actor_id).runs().list({ limit: 1 });
            if (runs.items.length === 0) {
                throw new Error('No runs found for actor');
            }
            datasetId = runs.items[0].defaultDatasetId;
        }
        
        console.log(`üì• Fetching from Apify dataset: ${datasetId}`);
        
        const dataset = client.dataset(datasetId);
        const { items } = await dataset.listItems();
        
        console.log(`üìä Processing ${items.length} records...`);
        
        for (const record of items) {
            result.recordsProcessed++;
            
            const { status, recordId } = await processRecord(record, config);
            
            switch (status) {
                case 'inserted':
                    result.recordsInserted++;
                    if (recordId) {
                        const entityId = await createEntityForRecord(recordId);
                        if (entityId) result.entitiesCreated++;
                    }
                    break;
                case 'updated':
                    result.recordsUpdated++;
                    break;
                case 'skipped':
                    result.recordsSkipped++;
                    break;
                case 'error':
                    result.recordsErrored++;
                    break;
            }
            
            if (result.recordsProcessed % 500 === 0) {
                console.log(`  ‚úì ${result.recordsProcessed.toLocaleString()} processed...`);
            }
        }
        
        result.success = true;
        
    } catch (error: any) {
        result.error = error.message;
        console.error('Sync failed:', error);
    }
    
    result.durationSeconds = Math.round((Date.now() - startTime) / 1000);
    
    // Update sync history
    await pool.query(`
        UPDATE apify_sync_history SET
            completed_at = NOW(),
            status = $2,
            records_processed = $3,
            records_inserted = $4,
            records_updated = $5,
            records_skipped = $6,
            records_errored = $7,
            duration_seconds = $8,
            error_message = $9
        WHERE id = $1
    `, [
        syncId,
        result.success ? 'completed' : 'failed',
        result.recordsProcessed,
        result.recordsInserted,
        result.recordsUpdated,
        result.recordsSkipped,
        result.recordsErrored,
        result.durationSeconds,
        result.error
    ]);
    
    // Update dataset
    await pool.query(`
        UPDATE apify_datasets SET 
            last_sync_at = NOW(),
            last_sync_record_count = $2,
            last_sync_error = $3
        WHERE id = $1
    `, [config.id, result.recordsProcessed, result.error]);
    
    return result;
}

// =====================================================================
// UTILITY EXPORTS
// =====================================================================

export async function getDatasetStatus() {
    const result = await pool.query(`SELECT * FROM v_dataset_sync_status`);
    return result.rows;
}

export async function getEntityStats() {
    const result = await pool.query(`SELECT * FROM v_entity_stats`);
    return result.rows;
}
Part 3: API Routes
typescript// server/routes/apify.ts

import { Router } from 'express';
import { pool } from '../db';
import { syncFromApify, syncFromFile, getDatasetStatus, getEntityStats } from '../services/apifySync';

const router = Router();

// GET /api/apify/datasets
router.get('/datasets', async (req, res) => {
    try {
        const datasets = await getDatasetStatus();
        res.json({ success: true, datasets });
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

// POST /api/apify/sync/:slug
router.post('/sync/:slug', async (req, res) => {
    try {
        const { slug } = req.params;
        const { filePath } = req.body;
        
        console.log(`üöÄ Starting sync for ${slug}...`);
        
        const result = filePath 
            ? await syncFromFile(slug, filePath)
            : await syncFromApify(slug);
        
        console.log(`‚úÖ Sync completed:`, result);
        
        res.json({ success: true, result });
        
    } catch (error: any) {
        console.error('Sync error:', error);
        res.status(500).json({ success: false, error: error.message });
    }
});

// GET /api/apify/stats
router.get('/stats', async (req, res) => {
    try {
        const stats = await getEntityStats();
        const totals = await pool.query(`
            SELECT 
                (SELECT COUNT(*) FROM external_records) as total_records,
                (SELECT COUNT(*) FROM entities) as total_entities,
                (SELECT COUNT(*) FROM entities WHERE claim_status = 'approved') as claimed_entities
        `);
        
        res.json({ success: true, stats, totals: totals.rows[0] });
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

// GET /api/apify/records
router.get('/records', async (req, res) => {
    try {
        const { source, community, search, limit = 50, offset = 0 } = req.query;
        
        let query = `
            SELECT er.*, c.name as community_name
            FROM external_records er
            LEFT JOIN sr_communities c ON c.id = er.community_id
            WHERE 1=1
        `;
        const params: any[] = [];
        let idx = 1;
        
        if (source) {
            query += ` AND er.source = $${idx++}::data_source_type`;
            params.push(source);
        }
        if (community) {
            query += ` AND er.community_id = $${idx++}`;
            params.push(community);
        }
        if (search) {
            query += ` AND er.name ILIKE $${idx++}`;
            params.push(`%${search}%`);
        }
        
        query += ` ORDER BY er.last_seen_at DESC NULLS LAST LIMIT $${idx++} OFFSET $${idx++}`;
        params.push(limit, offset);
        
        const result = await pool.query(query, params);
        res.json({ success: true, records: result.rows });
        
    } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
    }
});

export default router;

Install Dependencies
bashnpm install stream-json apify-client

How to Import
Option 1: Upload JSON + Run Script
bash# Upload 343MB file to ./data/apify-airbnb-bc.json
# Then call API:
curl -X POST http://localhost:5000/api/apify/sync/airbnb-bc \
  -H "Content-Type: application/json" \
  -d '{"filePath": "./data/apify-airbnb-bc.json"}'
Option 2: Direct from Apify API
bash# Set APIFY_API_TOKEN in environment
curl -X POST http://localhost:5000/api/apify/sync/airbnb-bc
This reconciles both approaches - keeping your V2 contact points architecture while adding the rental-specific fields and sync capability. üéØ